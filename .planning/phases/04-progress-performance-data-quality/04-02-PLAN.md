---
phase: 04-progress-performance-data-quality
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - src/config/cpu_detection.py
  - src/config/worker_scaling.py
  - src/config/settings.py
  - src/app.py
  - requirements.txt
autonomous: true

must_haves:
  truths:
    - "Worker count adapts to detected CPU count and available memory instead of using hardcoded defaults"
    - "Container environments detect cgroup CPU limits instead of host CPU count"
    - "Environment variable overrides for DISCOVERY_MAX_WORKERS and RESEARCH_MAX_WORKERS still work"
    - "Discovery and research multipliers are reduced to avoid wasteful over-sampling"
  artifacts:
    - path: "src/config/cpu_detection.py"
      provides: "Container-aware CPU detection with cgroup v1/v2 support"
      contains: "detect_cpu_limit"
    - path: "src/config/worker_scaling.py"
      provides: "Adaptive worker count calculation based on CPU and memory"
      contains: "calculate_worker_counts"
    - path: "src/config/settings.py"
      provides: "Adaptive worker counts and configurable multipliers"
      contains: "DISCOVERY_MULTIPLIER"
    - path: "requirements.txt"
      provides: "psutil dependency"
      contains: "psutil"
  key_links:
    - from: "src/config/worker_scaling.py"
      to: "src/config/cpu_detection.py"
      via: "calculate_worker_counts calls detect_cpu_limit"
      pattern: "detect_cpu_limit"
    - from: "src/config/settings.py"
      to: "src/config/worker_scaling.py"
      via: "DISCOVERY_MAX_WORKERS and RESEARCH_MAX_WORKERS computed by calculate_worker_counts"
      pattern: "calculate_worker_counts"
    - from: "src/app.py"
      to: "src/config/settings.py"
      via: "Pipeline uses DISCOVERY_MULTIPLIER and RESEARCH_MULTIPLIER for candidate counts"
      pattern: "DISCOVERY_MULTIPLIER|RESEARCH_MULTIPLIER"
---

<objective>
Implement adaptive worker scaling based on container CPU limits and available memory, and reduce pipeline multipliers to eliminate wasteful over-sampling of candidates.

Purpose: Fixed worker counts cause resource exhaustion in containers (detecting 64 host CPUs in a 2-CPU container) and waste API calls by researching 3-5x more suburbs than needed.

Output: Container-aware CPU detection module, adaptive worker scaling, configurable pipeline multipliers.

**Note on PERF-04 "pipeline optimization":** The primary performance bottleneck identified in research is wasteful over-samplingâ€”discovering 5x and researching 3x more suburbs than the user requested creates unnecessary Perplexity API calls. Reducing multipliers from 5/3 to 2.0/1.5 eliminates 60-80% of redundant API traffic, which IS the core pipeline optimization. This is not just "configuration tuning" but the main performance improvement needed.
</objective>

<execution_context>
@/Users/robertli/.claude/get-shit-done/workflows/execute-plan.md
@/Users/robertli/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-progress-performance-data-quality/04-RESEARCH.md
@src/config/settings.py
@src/app.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Container-aware CPU detection and adaptive worker scaling</name>
  <files>
    src/config/cpu_detection.py
    src/config/worker_scaling.py
    requirements.txt
  </files>
  <action>
1. Add `psutil>=6.0.0` to requirements.txt and install it.

2. Create `src/config/cpu_detection.py`:
   ```python
   """Container-aware CPU limit detection.

   Detects CPU limits from cgroup v2 (/sys/fs/cgroup/cpu.max) and
   cgroup v1 (/sys/fs/cgroup/cpu/cpu.cfs_quota_us) before falling
   back to os.cpu_count().
   """
   import os
   import logging
   from typing import Optional

   logger = logging.getLogger(__name__)

   def detect_cpu_limit() -> int:
       """Detect available CPUs, respecting container cgroup limits. Returns min 1."""
       # Try cgroup v2 first (newer Docker)
       result = _read_cgroup_v2_cpu()
       if result is not None:
           logger.info(f"CPU limit detected via cgroup v2: {result}")
           return result

       # Try cgroup v1
       result = _read_cgroup_v1_cpu()
       if result is not None:
           logger.info(f"CPU limit detected via cgroup v1: {result}")
           return result

       # Fallback to os.cpu_count()
       count = os.cpu_count() or 1
       logger.info(f"CPU count from os.cpu_count(): {count}")
       return count

   def _read_cgroup_v2_cpu() -> Optional[int]:
       """Read from /sys/fs/cgroup/cpu.max. Format: '$MAX $PERIOD' or 'max $PERIOD'."""
       try:
           with open('/sys/fs/cgroup/cpu.max', 'r') as f:
               parts = f.read().strip().split()
               if parts[0] == 'max':
                   return None  # Unlimited
               return max(1, int(int(parts[0]) / int(parts[1])))
       except (FileNotFoundError, ValueError, IndexError):
           return None

   def _read_cgroup_v1_cpu() -> Optional[int]:
       """Read from cgroup v1 quota/period files."""
       try:
           with open('/sys/fs/cgroup/cpu/cpu.cfs_quota_us', 'r') as f:
               quota = int(f.read().strip())
           if quota == -1:
               return None  # Unlimited
           with open('/sys/fs/cgroup/cpu/cpu.cfs_period_us', 'r') as f:
               period = int(f.read().strip())
           return max(1, int(quota / period))
       except (FileNotFoundError, ValueError):
           return None
   ```

3. Create `src/config/worker_scaling.py`:
   ```python
   """Adaptive worker count calculation based on CPU and memory."""
   import logging
   from typing import Optional, Tuple

   logger = logging.getLogger(__name__)

   # Caps to prevent runaway thread creation
   MAX_DISCOVERY_WORKERS = 8
   MAX_RESEARCH_WORKERS = 6

   def calculate_worker_counts(
       override_discovery: Optional[int] = None,
       override_research: Optional[int] = None
   ) -> Tuple[int, int]:
       """Calculate optimal worker counts based on CPU count and available memory.

       For I/O-bound work (API calls), uses 3x CPU for discovery (lighter),
       2x CPU for research (heavier). Memory-aware: reserves 2GB for system,
       assumes 500MB per worker.

       Returns: (discovery_workers, research_workers)
       """
       from src.config.cpu_detection import detect_cpu_limit

       cpu_count = detect_cpu_limit()

       # Base: I/O-bound multipliers with caps
       base_discovery = min(cpu_count * 3, MAX_DISCOVERY_WORKERS)
       base_research = min(cpu_count * 2, MAX_RESEARCH_WORKERS)

       # Memory-aware adjustment using psutil
       try:
           import psutil
           available_gb = psutil.virtual_memory().available / (1024 ** 3)
           max_by_memory = max(1, int((available_gb - 2) / 0.5))
           base_discovery = min(base_discovery, max_by_memory)
           base_research = min(base_research, max_by_memory)
           logger.info(f"Worker scaling: CPU={cpu_count}, Memory={available_gb:.1f}GB available")
       except ImportError:
           logger.warning("psutil not available, skipping memory-based scaling")

       # Apply overrides
       discovery = override_discovery if override_discovery is not None else base_discovery
       research = override_research if override_research is not None else base_research

       # Ensure minimum 1
       discovery = max(1, discovery)
       research = max(1, research)

       logger.info(f"Worker counts: discovery={discovery}, research={research}")
       return discovery, research
   ```
  </action>
  <verify>
    - `pip install psutil` succeeds
    - `python -c "from src.config.cpu_detection import detect_cpu_limit; print(f'CPUs: {detect_cpu_limit()}')"` prints a number >= 1
    - `python -c "from src.config.worker_scaling import calculate_worker_counts; d, r = calculate_worker_counts(); print(f'discovery={d}, research={r}')"` prints reasonable values
    - `python -c "from src.config.worker_scaling import calculate_worker_counts; d, r = calculate_worker_counts(override_discovery=2, override_research=1); assert d == 2 and r == 1; print('overrides work')"` passes
  </verify>
  <done>cpu_detection.py reads cgroup v2/v1 with fallback to os.cpu_count(). worker_scaling.py computes adaptive counts with memory awareness and override support. psutil is installed.</done>
</task>

<task type="auto">
  <name>Task 2: Wire adaptive scaling into settings and reduce pipeline multipliers</name>
  <files>
    src/config/settings.py
    src/app.py
  </files>
  <action>
1. In `src/config/settings.py`:
   - Replace the existing hardcoded `DISCOVERY_MAX_WORKERS = int(os.getenv("DISCOVERY_MAX_WORKERS", "4"))` and `RESEARCH_MAX_WORKERS = int(os.getenv("RESEARCH_MAX_WORKERS", "3"))` lines with adaptive scaling, using try/except to handle import failures gracefully:
     ```python
     # Adaptive worker scaling with fallback to hardcoded defaults if dependencies fail
     try:
         from src.config.worker_scaling import calculate_worker_counts

         # Parse environment overrides (None if not set, int if set)
         _discovery_override = int(os.getenv("DISCOVERY_MAX_WORKERS")) if os.getenv("DISCOVERY_MAX_WORKERS") else None
         _research_override = int(os.getenv("RESEARCH_MAX_WORKERS")) if os.getenv("RESEARCH_MAX_WORKERS") else None

         DISCOVERY_MAX_WORKERS, RESEARCH_MAX_WORKERS = calculate_worker_counts(
             override_discovery=_discovery_override,
             override_research=_research_override,
         )
     except (ImportError, Exception) as e:
         # Fallback to safe defaults if worker_scaling or dependencies fail
         import logging
         logging.warning(f"Failed to load adaptive worker scaling, using defaults: {e}")
         DISCOVERY_MAX_WORKERS = int(os.getenv("DISCOVERY_MAX_WORKERS", "4"))
         RESEARCH_MAX_WORKERS = int(os.getenv("RESEARCH_MAX_WORKERS", "3"))
     ```
   - Add configurable pipeline multipliers:
     ```python
     # Pipeline multipliers: how many extra candidates to discover/research beyond what user requested
     # Reduced from 5/3 to 2.0/1.5 per PERF-04: eliminates 60-80% of wasteful API calls
     DISCOVERY_MULTIPLIER = float(os.getenv("DISCOVERY_MULTIPLIER", "2.0"))  # Down from 5
     RESEARCH_MULTIPLIER = float(os.getenv("RESEARCH_MULTIPLIER", "1.5"))    # Down from 3
     ```
   - Add ranking quality weights (for plan 04-03 to consume, but define here for configuration centralization):
     ```python
     # Ranking quality weights
     RANKING_QUALITY_WEIGHTS = {
         "high": float(os.getenv("QUALITY_WEIGHT_HIGH", "1.0")),
         "medium": float(os.getenv("QUALITY_WEIGHT_MEDIUM", "0.95")),
         "low": float(os.getenv("QUALITY_WEIGHT_LOW", "0.85")),
         "fallback": float(os.getenv("QUALITY_WEIGHT_FALLBACK", "0.70")),
     }
     DEFAULT_RANKING_METHOD = os.getenv("DEFAULT_RANKING_METHOD", "quality_adjusted")
     ```

2. In `src/app.py`:
   - Find the line where `max_results=user_input.num_suburbs * 5` (or similar multiplier) is used in the `parallel_discover_suburbs()` call
   - Replace with: `max_results=int(user_input.num_suburbs * settings.DISCOVERY_MULTIPLIER)`
   - Find the line where `research_count = min(len(candidates), user_input.num_suburbs * 3)` (or similar)
   - Replace with: `research_count = min(len(candidates), int(user_input.num_suburbs * settings.RESEARCH_MULTIPLIER))`
   - Import settings if not already imported: `from src.config import settings`
  </action>
  <verify>
    - `grep -n "DISCOVERY_MULTIPLIER\|RESEARCH_MULTIPLIER" src/config/settings.py` shows both multiplier settings
    - `grep -n "calculate_worker_counts" src/config/settings.py` shows adaptive scaling is wired in
    - `grep -n "try:" src/config/settings.py` shows try/except wrapper around adaptive scaling import
    - `grep -n "DISCOVERY_MULTIPLIER\|RESEARCH_MULTIPLIER" src/app.py` shows multipliers used in pipeline
    - `python -c "from src.config import settings; print(f'D={settings.DISCOVERY_MAX_WORKERS}, R={settings.RESEARCH_MAX_WORKERS}, DM={settings.DISCOVERY_MULTIPLIER}, RM={settings.RESEARCH_MULTIPLIER}')"` prints reasonable values
    - No hardcoded `* 5` or `* 3` multipliers remain in app.py for suburb counts
  </verify>
  <done>settings.py uses adaptive worker counts with try/except fallback to prevent import failures from crashing the app. Pipeline multipliers reduced from 5/3 to 2.0/1.5 (PERF-04 optimization) and are configurable via env vars. Ranking quality weights defined for downstream use.</done>
</task>

</tasks>

<verification>
1. Worker scaling adapts: `python -c "from src.config.worker_scaling import calculate_worker_counts; print(calculate_worker_counts())"` returns values based on actual CPU/memory
2. Env overrides work: `DISCOVERY_MAX_WORKERS=2 python -c "from src.config import settings; print(settings.DISCOVERY_MAX_WORKERS)"` prints 2
3. Multipliers configurable: `DISCOVERY_MULTIPLIER=3.0 python -c "from src.config import settings; print(settings.DISCOVERY_MULTIPLIER)"` prints 3.0
4. No import errors: `python -c "from src.app import run_research_pipeline; print('OK')"` succeeds
5. Fallback works: If psutil is unavailable, settings.py still loads with default worker counts
</verification>

<success_criteria>
- Container CPU detection reads cgroup v2/v1 with os.cpu_count() fallback
- Worker counts scale with hardware (capped at 8 discovery, 6 research)
- Environment variable overrides still function for both workers and multipliers
- Pipeline multipliers reduced from 5/3 to 2.0/1.5 to save API calls (PERF-04)
- Import failures in worker scaling do not crash settings.py (fallback to defaults)
</success_criteria>

<output>
After completion, create `.planning/phases/04-progress-performance-data-quality/04-02-SUMMARY.md`
</output>
