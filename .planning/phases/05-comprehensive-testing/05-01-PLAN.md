---
phase: 05-comprehensive-testing
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - requirements-dev.txt
  - tests/conftest.py
  - tests/unit/test_cache.py
  - tests/unit/test_exceptions.py
  - tests/unit/test_validation.py
  - tests/unit/test_worker_scaling.py
  - tests/fixtures/mock_responses.py
  - tests/fixtures/sample_data.py
autonomous: true

must_haves:
  truths:
    - "Cache get/put/invalidate/clear operations produce correct results in isolation"
    - "Every exception type (RateLimitError, TimeoutError, AuthenticationError, ValidationError, etc.) carries correct metadata and routes to the correct handler category"
    - "Discovery and research API responses with string prices, missing fields, and type mismatches are correctly coerced or rejected with field-level warnings"
    - "Worker scaling respects CPU limits, memory constraints, and override parameters"
  artifacts:
    - path: "pyproject.toml"
      provides: "pytest configuration with markers, asyncio_mode, coverage settings"
      contains: "tool.pytest.ini_options"
    - path: "requirements-dev.txt"
      provides: "Test dependencies"
      contains: "pytest"
    - path: "tests/conftest.py"
      provides: "Shared fixtures: temp_cache_dir, research_cache, reset_globals"
      min_lines: 40
    - path: "tests/unit/test_cache.py"
      provides: "Cache CRUD, expiry, backup recovery, orphan cleanup, LRU eviction tests"
      min_lines: 100
    - path: "tests/unit/test_exceptions.py"
      provides: "Exception hierarchy, metadata, isinstance routing tests"
      min_lines: 60
    - path: "tests/unit/test_validation.py"
      provides: "Pydantic validation coercion, rejection, warning generation tests"
      min_lines: 80
    - path: "tests/unit/test_worker_scaling.py"
      provides: "CPU detection, memory scaling, override tests"
      min_lines: 40
  key_links:
    - from: "tests/conftest.py"
      to: "src/research/cache.py"
      via: "CacheConfig + ResearchCache fixture"
      pattern: "ResearchCache"
    - from: "tests/unit/test_validation.py"
      to: "src/research/validation.py"
      via: "validate_discovery_response, validate_research_response imports"
      pattern: "validate_(discovery|research)_response"
---

<objective>
Set up pytest infrastructure and write unit tests covering cache operations, exception hierarchy, API response validation, and adaptive worker scaling.

Purpose: Establishes the test foundation (pytest config, shared fixtures, test data) and validates the four core unit-testable subsystems built in Phases 1-4. Covers requirements TEST-01, TEST-02, TEST-03.
Output: Working pytest suite with 40+ unit tests passing via `pytest -m unit`
</objective>

<execution_context>
@/Users/robertli/.claude/get-shit-done/workflows/execute-plan.md
@/Users/robertli/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-comprehensive-testing/05-RESEARCH.md
@src/research/cache.py
@src/security/exceptions.py
@src/research/validation.py
@src/config/worker_scaling.py
@src/config/cpu_detection.py
@src/security/validators.py
@src/research/ranking.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Set up pytest infrastructure, fixtures, and test data</name>
  <files>
    pyproject.toml
    requirements-dev.txt
    tests/conftest.py
    tests/__init__.py
    tests/unit/__init__.py
    tests/fixtures/__init__.py
    tests/fixtures/mock_responses.py
    tests/fixtures/sample_data.py
  </files>
  <action>
1. Create `requirements-dev.txt` with test dependencies:
   - pytest>=8.0.0, pytest-asyncio>=0.24.0, pytest-mock>=3.14.0, pytest-cov>=6.0.0
   - httpx>=0.27.0, responses>=0.25.0, freezegun>=1.5.0
   - Do NOT include faker or pytest-repeat (unnecessary for this scope)

2. Add pytest configuration to `pyproject.toml` (create file if it does not exist, otherwise add section):
   ```toml
   [tool.pytest.ini_options]
   markers = [
       "unit: Fast unit tests with no external dependencies",
       "integration: Integration tests spanning multiple components",
       "asyncio: Async tests using pytest-asyncio",
       "concurrent: Thread safety and race condition tests",
       "slow: Tests that take > 5 seconds",
   ]
   asyncio_mode = "auto"
   testpaths = ["tests"]
   addopts = "--strict-markers --tb=short -q"
   pythonpath = ["src"]

   [tool.coverage.run]
   source = ["src"]
   omit = ["*/tests/*", "*/__pycache__/*", "*/ui/web/templates/*", "*/ui/web/static/*"]

   [tool.coverage.report]
   exclude_lines = [
       "pragma: no cover",
       "def __repr__",
       "raise NotImplementedError",
       "if __name__ == .__main__.",
   ]
   ```

3. Create `tests/conftest.py` with shared fixtures:
   - `temp_cache_dir`: function-scoped TemporaryDirectory yielding Path
   - `cache_config(temp_cache_dir)`: CacheConfig with temp dir, short TTLs (60s discovery, 120s research)
   - `research_cache(cache_config)`: ResearchCache instance
   - `reset_cache_singleton()`: autouse fixture that calls reset_cache_instance() after each test
   - Important: Set PYTHONPATH so `from research.cache import ...` etc. works (pythonpath in pyproject.toml handles this)

4. Create `tests/__init__.py`, `tests/unit/__init__.py`, `tests/fixtures/__init__.py` (empty files).

5. Create `tests/fixtures/sample_data.py` with factory functions:
   - `make_discovery_suburb(**overrides)` -> dict: Returns valid discovery suburb dict with defaults (name="TestSuburb", state="QLD", lga="Brisbane", median_price=500000)
   - `make_research_response(**overrides)` -> dict: Returns valid full research response dict with all sections (identification, market_current, market_history, physical_config, demographics, infrastructure, growth_projections)
   - `make_suburb_metrics(**overrides)` -> SuburbMetrics: Returns SuburbMetrics object from sample research data

6. Create `tests/fixtures/mock_responses.py`:
   - `VALID_DISCOVERY_RESPONSE`: List of 3 valid suburb dicts
   - `MALFORMED_DISCOVERY_RESPONSE`: List with string prices ("500000"), missing fields, empty names
   - `VALID_RESEARCH_RESPONSE`: Complete research response dict
   - `PARTIAL_RESEARCH_RESPONSE`: Response with required fields only, optional sections empty/null
   - `INVALID_RESEARCH_RESPONSE`: Response missing identification.name (required field)

Install deps: `pip install -r requirements-dev.txt`
  </action>
  <verify>
Run `pip install -r requirements-dev.txt` succeeds. Run `python -m pytest --co -q tests/` shows test collection works (0 tests collected is fine at this stage). Verify `python -c "from tests.fixtures.sample_data import make_discovery_suburb; print(make_discovery_suburb())"` outputs a valid dict.
  </verify>
  <done>
pyproject.toml has [tool.pytest.ini_options] section. requirements-dev.txt exists with all test deps installed. conftest.py has temp_cache_dir, cache_config, research_cache, and reset_cache_singleton fixtures. sample_data.py and mock_responses.py provide reusable test data factories and canned responses.
  </done>
</task>

<task type="auto">
  <name>Task 2: Write unit tests for cache, exceptions, validation, and worker scaling</name>
  <files>
    tests/unit/test_cache.py
    tests/unit/test_exceptions.py
    tests/unit/test_validation.py
    tests/unit/test_worker_scaling.py
  </files>
  <action>
1. `tests/unit/test_cache.py` -- Mark all tests with `@pytest.mark.unit`. Test the following:
   - `test_put_and_get`: Put a discovery entry, get it back, verify data matches
   - `test_get_missing_key`: Get nonexistent key returns None
   - `test_get_expired_entry`: Use freezegun to freeze time, put entry, advance time past TTL, verify get returns None
   - `test_put_overwrites_existing`: Put same key twice with different data, verify second data returned
   - `test_invalidate_existing`: Put entry, invalidate it, verify get returns None and invalidate returned True
   - `test_invalidate_nonexistent`: Invalidate nonexistent key returns False
   - `test_clear_all`: Put 3 entries (2 discovery, 1 research), clear all, verify stats show 0
   - `test_clear_by_type`: Put 2 discovery + 1 research, clear("discovery"), verify research entry still exists
   - `test_stats_counts`: Put entries of both types, verify stats dict has correct counts
   - `test_backup_recovery`: Put entries, corrupt the main index file (write invalid JSON), create new ResearchCache with same config, verify entries recovered from backup
   - `test_orphan_cleanup`: Create a stray file `discovery_orphan123.json` in cache dir, create new ResearchCache, verify orphan file deleted
   - `test_lru_eviction`: Set max_size_bytes very small (1024), put multiple entries, verify oldest-accessed evicted. Mock `settings.CACHE_MAX_SIZE_MB` to a tiny value.
   - `test_atomic_write_creates_file`: Call atomic_write_json directly, verify file exists with correct content
   - `test_make_key_deterministic`: Same inputs produce same hash, different inputs produce different hashes
   - `test_bucket_price`: bucket_price(475000) == 500000, bucket_price(525000) == 500000, bucket_price(550000) == 550000
   - `test_disabled_cache_returns_none`: Create cache with enabled=False, put data, get returns None

2. `tests/unit/test_exceptions.py` -- Mark all with `@pytest.mark.unit`. Test:
   - `test_application_error_metadata`: Create ApplicationError with all kwargs, verify error_code, retry_after, is_transient, provider attributes
   - `test_permanent_error_not_transient`: PermanentError.is_transient is always False
   - `test_validation_error_has_field`: ValidationError("bad input", field="regions") has field="regions" and error_code="VALIDATION_ERROR"
   - `test_auth_error_has_provider`: AuthenticationError("bad key", provider="perplexity") has provider="perplexity"
   - `test_rate_limit_default_retry`: RateLimitError("slow down") has retry_after=60 by default, is_transient=True
   - `test_timeout_error_has_duration`: TimeoutError("timed out", timeout_seconds=30) has timeout_seconds=30
   - `test_api_error_has_status_code`: APIError("server error", status_code=503) has status_code=503
   - `test_cache_error_has_operation`: CacheError("write failed", operation="write") has operation="write"
   - `test_transient_errors_tuple`: Verify TRANSIENT_ERRORS contains RateLimitError, TimeoutError, NetworkError, APIError
   - `test_permanent_errors_tuple`: Verify PERMANENT_ERRORS contains AuthenticationError, ConfigurationError, ValidationError
   - `test_isinstance_routing`: Use parametrize with each error type, verify isinstance checks against TransientError vs PermanentError correctly classify them. This is the key test -- demonstrate that `isinstance(RateLimitError(...), TransientError)` is True and `isinstance(RateLimitError(...), PermanentError)` is False, etc.
   - `test_str_returns_message`: str(ApplicationError("test message")) returns "test message" (not metadata)

3. `tests/unit/test_validation.py` -- Mark all with `@pytest.mark.unit`. Test:
   - `test_coerce_numeric_string_int`: coerce_numeric("450000") == 450000
   - `test_coerce_numeric_string_float`: coerce_numeric("3.5") == 3.5
   - `test_coerce_numeric_none`: coerce_numeric(None) is None
   - `test_coerce_numeric_empty_string`: coerce_numeric("") is None
   - `test_coerce_numeric_invalid_string`: coerce_numeric("not_a_number") is None
   - `test_coerce_numeric_passthrough_int`: coerce_numeric(42) == 42
   - `test_coerce_numeric_passthrough_float`: coerce_numeric(3.14) == 3.14
   - `test_coerce_numeric_zero`: coerce_numeric(0) == 0
   - `test_discovery_valid_response`: Pass VALID_DISCOVERY_RESPONSE through validate_discovery_response, verify is_valid=True, len(data)==3, no warnings
   - `test_discovery_string_prices_coerced`: Pass list with one suburb having median_price="500000" (string), verify coerced to 500000.0
   - `test_discovery_missing_required_field`: Pass suburb dict without "name", verify excluded from results and warning generated
   - `test_discovery_invalid_state`: Pass suburb with state="INVALID", verify excluded with warning
   - `test_discovery_empty_list_invalid`: Pass empty list, verify is_valid=False and "No valid suburbs" warning
   - `test_discovery_mixed_valid_invalid`: Pass 3 suburbs where 1 is invalid, verify 2 valid items returned with 1 warning
   - `test_research_valid_response`: Pass VALID_RESEARCH_RESPONSE through validate_research_response, verify is_valid=True
   - `test_research_partial_response`: Pass PARTIAL_RESEARCH_RESPONSE (only required fields), verify is_valid=True with warnings about missing optional data
   - `test_research_missing_identification_raises`: Pass response without identification section, verify AppValidationError raised
   - `test_research_missing_median_price_raises`: Pass response with identification but no market_current.median_price, verify AppValidationError raised
   - `test_research_string_price_coerced`: Pass research response with median_price="600000", verify coerced to 600000.0
   - `test_growth_projections_string_keys_coerced`: Pass projected_growth_pct with string keys {"1": 5.0, "5": 25.0}, verify keys coerced to ints
   - `test_data_quality_validation`: Pass data_quality="invalid_value", verify defaults to "medium"

   Import test data from `tests.fixtures.mock_responses`.

4. `tests/unit/test_worker_scaling.py` -- Mark all with `@pytest.mark.unit`. Test:
   - `test_override_discovery_workers`: calculate_worker_counts(override_discovery=4) returns (4, _)
   - `test_override_research_workers`: calculate_worker_counts(override_research=3) returns (_, 3)
   - `test_minimum_one_worker`: calculate_worker_counts(override_discovery=0, override_research=0) returns (1, 1)
   - `test_caps_respected`: Mock detect_cpu_limit to return 100, verify discovery <= MAX_DISCOVERY_WORKERS (8) and research <= MAX_RESEARCH_WORKERS (6)
   - `test_cpu_detection_fallback`: Mock cgroup files to not exist, verify detect_cpu_limit returns os.cpu_count() result
   - `test_cgroup_v2_parsing`: Mock /sys/fs/cgroup/cpu.max with "200000 100000", verify detect_cpu_limit returns 2
   - `test_cgroup_v2_unlimited`: Mock /sys/fs/cgroup/cpu.max with "max 100000", verify falls through to next method
   - `test_memory_scaling`: Mock psutil.virtual_memory().available to 4GB (4*1024^3), verify workers are capped by memory formula: max(1, int((4-2)/0.5)) = 4

   Use mocker (pytest-mock) to mock detect_cpu_limit, psutil, and file reads. For cgroup tests, use mocker.patch("builtins.open", ...) with mock_open to simulate /sys/fs/cgroup/cpu.max content.
  </action>
  <verify>
Run `python -m pytest tests/unit/ -v --tb=short`. All tests pass. Run `python -m pytest tests/unit/ --cov=src --cov-report=term-missing -q` to see coverage of tested modules. Expect cache.py, exceptions.py, validation.py, worker_scaling.py, cpu_detection.py all have >70% coverage.
  </verify>
  <done>
40+ unit tests pass across 4 test files. Cache operations tested (CRUD, expiry, backup recovery, orphan cleanup, LRU eviction). Exception hierarchy tested (metadata, isinstance routing, transient vs permanent classification). Validation tested (coercion, rejection, warnings for discovery and research). Worker scaling tested (CPU detection, memory scaling, overrides, caps).
  </done>
</task>

</tasks>

<verification>
```bash
# All unit tests pass
python -m pytest tests/unit/ -v --tb=short -m unit

# Coverage report
python -m pytest tests/unit/ --cov=src/research/cache --cov=src/security/exceptions --cov=src/research/validation --cov=src/config/worker_scaling --cov=src/config/cpu_detection --cov-report=term-missing

# No import errors
python -c "from tests.fixtures.sample_data import make_discovery_suburb, make_research_response; print('OK')"
python -c "from tests.fixtures.mock_responses import VALID_DISCOVERY_RESPONSE, VALID_RESEARCH_RESPONSE; print('OK')"
```
</verification>

<success_criteria>
- pytest collects and runs 40+ unit tests with 0 failures
- Cache tests verify CRUD, expiry, backup recovery, orphan cleanup, LRU eviction
- Exception tests verify isinstance routing works for all error types (transient vs permanent)
- Validation tests verify coercion of string prices, rejection of missing required fields, field-level warnings
- Worker scaling tests verify CPU detection, memory-aware scaling, override parameters, and caps
- Coverage of cache.py, exceptions.py, validation.py, worker_scaling.py, cpu_detection.py is >70%
</success_criteria>

<output>
After completion, create `.planning/phases/05-comprehensive-testing/05-01-SUMMARY.md`
</output>
