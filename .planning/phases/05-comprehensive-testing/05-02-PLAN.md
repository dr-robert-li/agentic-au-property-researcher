---
phase: 05-comprehensive-testing
plan: 02
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - tests/integration/__init__.py
  - tests/integration/test_pipeline.py
  - tests/async_tests/__init__.py
  - tests/async_tests/test_sse_endpoints.py
  - tests/concurrent/__init__.py
  - tests/concurrent/test_thread_safety.py
autonomous: true

must_haves:
  truths:
    - "End-to-end pipeline test runs discovery through ranking with mocked API and produces ranked SuburbReport objects"
    - "SSE endpoint streams progress events and handles client disconnect cleanup"
    - "Concurrent cache writes from 10 threads produce no corruption or lost entries"
    - "Web server shared state (active_runs, completed_runs) is safe under concurrent access"
  artifacts:
    - path: "tests/integration/test_pipeline.py"
      provides: "Discovery-to-ranking pipeline integration test with mocked Perplexity responses"
      min_lines: 80
    - path: "tests/async_tests/test_sse_endpoints.py"
      provides: "SSE streaming, disconnect cleanup, reconnection tests"
      min_lines: 60
    - path: "tests/concurrent/test_thread_safety.py"
      provides: "Multi-threaded cache writes, server state contention, queue thread safety tests"
      min_lines: 80
  key_links:
    - from: "tests/integration/test_pipeline.py"
      to: "src/research/suburb_discovery.py"
      via: "mocked discover_suburbs call"
      pattern: "discover_suburbs|research_suburb"
    - from: "tests/async_tests/test_sse_endpoints.py"
      to: "src/ui/web/server.py"
      via: "httpx.AsyncClient with ASGITransport"
      pattern: "api/progress.*stream"
    - from: "tests/concurrent/test_thread_safety.py"
      to: "src/research/cache.py"
      via: "ThreadPoolExecutor with threading.Barrier"
      pattern: "Barrier|ThreadPoolExecutor"
---

<objective>
Write integration tests for the discovery-to-research pipeline, async tests for SSE endpoints, and concurrent thread safety tests for cache and server state.

Purpose: Validates that multi-component interactions work correctly (pipeline), async SSE streaming behaves properly (events, disconnect, reconnect), and thread safety holds under concurrent load. Covers requirements TEST-04, TEST-05, TEST-06.
Output: Integration, async, and concurrent test suites passing via `pytest -m "integration or asyncio or concurrent"`
</objective>

<execution_context>
@/Users/robertli/.claude/get-shit-done/workflows/execute-plan.md
@/Users/robertli/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-comprehensive-testing/05-RESEARCH.md
@.planning/phases/05-comprehensive-testing/05-01-SUMMARY.md
@src/ui/web/server.py
@src/research/cache.py
@src/research/suburb_discovery.py
@src/research/suburb_research.py
@src/research/ranking.py
@src/research/validation.py
@src/models/suburb_metrics.py
@src/models/run_result.py
@tests/conftest.py
@tests/fixtures/mock_responses.py
@tests/fixtures/sample_data.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Integration test for discovery-to-ranking pipeline</name>
  <files>
    tests/integration/__init__.py
    tests/integration/test_pipeline.py
  </files>
  <action>
Create `tests/integration/__init__.py` (empty).

Create `tests/integration/test_pipeline.py` with the following tests, all marked `@pytest.mark.integration`:

The pipeline test must NOT call real APIs. Mock the Perplexity/Anthropic clients at the module level using pytest-mock's `mocker` fixture or `unittest.mock.patch`.

1. `test_discovery_returns_suburbs(mocker)`:
   - Mock the Perplexity client's deep_research call (or however discover_suburbs calls the API) to return a JSON string containing 5 valid suburb entries. Use VALID_DISCOVERY_RESPONSE from fixtures as the basis.
   - Call the `discover_suburbs` function (from `src/research/suburb_discovery.py`) with max_price=600000, dwelling_type="house", regions=["Queensland"]
   - Verify: Returns a list of dicts, each with "name", "state", "median_price" fields
   - Verify: All median_price values are <= 600000

2. `test_research_suburb_returns_metrics(mocker)`:
   - Mock the API client to return VALID_RESEARCH_RESPONSE (from fixtures) as the API output
   - Call `research_suburb` (from `src/research/suburb_research.py`) with a suburb name and config
   - Verify: Returns a SuburbMetrics object (or dict that can construct one)
   - Verify: identification.name matches input, market_current.median_price > 0

3. `test_ranking_produces_ordered_reports`:
   - Use `make_suburb_metrics` from sample_data to create 5 SuburbMetrics objects with different growth scores (e.g., 80, 60, 90, 70, 50)
   - Call `rank_suburbs(metrics_list, ranking_method="growth_score", top_n=3)`
   - Verify: Returns 3 SuburbReport objects
   - Verify: Reports are ordered by growth_score descending (90, 80, 70)
   - Verify: Each report has rank attribute set (1, 2, 3)

4. `test_quality_adjusted_ranking`:
   - Create 2 SuburbMetrics: one with growth_score=90/data_quality="low", one with growth_score=80/data_quality="high"
   - Rank with method="quality_adjusted"
   - Verify: The high-quality suburb ranks first (80 * 1.0 = 80 > 90 * 0.85 = 76.5)

5. `test_ranking_empty_list`:
   - rank_suburbs([]) returns empty list

6. `test_filter_by_criteria`:
   - Create 5 SuburbMetrics with various prices and states
   - Call filter_by_criteria with max_price=600000, states=["QLD"]
   - Verify only matching suburbs returned

7. `test_validation_wired_into_discovery(mocker)`:
   - Mock API to return MALFORMED_DISCOVERY_RESPONSE (mix of valid and invalid suburbs)
   - Call discover_suburbs (if it uses validate_discovery_response internally) or call validate_discovery_response directly with malformed data
   - Verify: Valid suburbs are kept, invalid ones excluded, warnings generated

Important implementation notes:
- Read the actual function signatures from `src/research/suburb_discovery.py` and `src/research/suburb_research.py` to understand the mock targets. The mock must patch the correct path (e.g., `research.suburb_discovery.PerplexityClient` or wherever the API call happens).
- If discover_suburbs/research_suburb take a `progress_callback`, pass a no-op lambda: `lambda msg, pct=0: None`
- Use `from tests.fixtures.sample_data import make_suburb_metrics` and `from tests.fixtures.mock_responses import *`
  </action>
  <verify>
Run `python -m pytest tests/integration/ -v --tb=short -m integration`. All tests pass. No real API calls made (verify no network errors if PERPLEXITY_API_KEY is unset).
  </verify>
  <done>
7 integration tests pass. Pipeline flows from discovery through validation to ranking with mocked API responses. Quality-adjusted ranking correctly penalizes low-quality data. Filter criteria work correctly.
  </done>
</task>

<task type="auto">
  <name>Task 2: Async SSE tests and concurrent thread safety tests</name>
  <files>
    tests/async_tests/__init__.py
    tests/async_tests/test_sse_endpoints.py
    tests/concurrent/__init__.py
    tests/concurrent/test_thread_safety.py
  </files>
  <action>
**Part A: SSE Endpoint Tests** (`tests/async_tests/test_sse_endpoints.py`)

Create `tests/async_tests/__init__.py` (empty).

All tests marked `@pytest.mark.asyncio`. Use `httpx.AsyncClient` with `ASGITransport(app=app)` from `src.ui.web.server`. The server module imports settings and other modules, so tests may need to mock or set up enough environment to import the app. If import fails due to missing .env/API keys, mock `src.config.settings` attributes before importing the app.

Important: Before importing the app, ensure `PERPLEXITY_API_KEY` env var is set to a dummy value like "test-key" so settings module doesn't fail at import. Use `monkeypatch.setenv("PERPLEXITY_API_KEY", "test-key")` in a session-scoped fixture, or set it in conftest.py.

Add to `tests/conftest.py` (append, don't overwrite):
```python
@pytest.fixture(autouse=True, scope="session")
def mock_env_vars():
    """Set dummy env vars so settings module loads without real API keys."""
    import os
    os.environ.setdefault("PERPLEXITY_API_KEY", "test-key-not-real")
    os.environ.setdefault("ANTHROPIC_API_KEY", "test-key-not-real")
```

Tests:

1. `test_health_endpoint`: GET /health returns 200 with "status": "healthy"

2. `test_sse_stream_not_found_run`: GET /api/progress/nonexistent-run/stream returns an SSE response with an error event containing "Run not found"

3. `test_sse_stream_delivers_progress_events`:
   - Directly create a progress_queue for a fake run_id in server.progress_queues
   - Put 3 progress messages into the queue, then put None (completion sentinel)
   - Connect via AsyncClient.stream("GET", "/api/progress/{run_id}/stream")
   - Read lines, verify at least 3 "event: progress" entries and 1 "event: complete"
   - Clean up: remove run_id from progress_queues

4. `test_sse_connection_cleanup`:
   - Create a progress_queue, register it in server.progress_queues
   - Put 1 message, do NOT put sentinel (stream stays open)
   - Connect and read 1 event, then disconnect (exit async with block)
   - After small delay, verify the run_id entry in sse_connections is cleaned up (empty set or removed)
   - Clean up: remove from progress_queues

5. `test_api_status_not_found`: GET /api/status/nonexistent returns {"error": "Run not found"}

6. `test_cache_stats_endpoint`: GET /cache/stats returns 200 with dict containing "total_entries" key

**Part B: Thread Safety Tests** (`tests/concurrent/test_thread_safety.py`)

Create `tests/concurrent/__init__.py` (empty).

All tests marked `@pytest.mark.concurrent`. Use the `research_cache` fixture from conftest.

1. `test_concurrent_cache_writes(research_cache)`:
   - Create threading.Barrier(10) to synchronize 10 threads
   - Each thread: barrier.wait(), then put a unique key+data, then get it back
   - Use ThreadPoolExecutor(max_workers=10)
   - Verify: All 10 threads succeed (no exceptions)
   - Verify: All 10 entries retrievable with correct data

2. `test_concurrent_reads_during_writes(research_cache)`:
   - Pre-populate cache with 5 entries
   - Spawn 5 writer threads (put new entries) + 5 reader threads (get existing entries) simultaneously
   - Use Barrier(10) to sync all threads
   - Verify: No exceptions raised
   - Verify: Readers always get either valid data or None (never corrupted partial data)

3. `test_concurrent_cache_invalidation(research_cache)`:
   - Pre-populate with 10 entries
   - Spawn 10 threads, each invalidating a different key, using Barrier(10)
   - Verify: All invalidations succeed
   - Verify: stats() shows 0 entries

4. `test_server_state_concurrent_access`:
   - Import active_runs, active_runs_lock, completed_runs, completed_runs_lock from server module
   - Spawn 10 threads that each add a run to active_runs (with lock), then move it to completed_runs (with lock)
   - Use Barrier(10)
   - Verify: After all threads complete, active_runs is empty, completed_runs has 10 entries
   - Clean up: Clear both dicts

5. `test_progress_queue_thread_safety`:
   - Create queue.Queue(maxsize=100)
   - Spawn 5 producer threads (each put 20 messages) + 1 consumer thread (get all messages)
   - Verify: Consumer receives exactly 100 messages total (5 * 20)
   - Verify: No exceptions

6. `test_deepcopy_isolation`:
   - Create shared state dict with nested mutable data
   - Use threading.Lock
   - Thread 1: Read with deepcopy, mutate the copy
   - Thread 2: Read with deepcopy, verify original state unchanged
   - This validates the pattern used in server.py for active_runs access

7. `test_singleton_cache_thread_safety(mocker)`:
   - Reset the singleton via reset_cache_instance()
   - Mock settings to provide valid cache config
   - Spawn 10 threads all calling get_cache() simultaneously (Barrier)
   - Verify: All threads get the same instance (same id())
   - Clean up: reset_cache_instance()
  </action>
  <verify>
Run `python -m pytest tests/async_tests/ -v --tb=short -m asyncio`. All SSE tests pass.
Run `python -m pytest tests/concurrent/ -v --tb=short -m concurrent`. All thread safety tests pass.
Run `python -m pytest tests/ -v --tb=short -q` to run the full suite (unit + integration + async + concurrent). All tests pass.
  </verify>
  <done>
6 async SSE tests pass (health, stream delivery, connection cleanup, status, cache stats). 7 concurrent thread safety tests pass (cache writes, reads-during-writes, invalidation, server state, queue safety, deepcopy isolation, singleton safety). Full test suite runs with 0 failures.
  </done>
</task>

</tasks>

<verification>
```bash
# Full test suite
python -m pytest tests/ -v --tb=short

# By category
python -m pytest tests/ -m unit -q
python -m pytest tests/ -m integration -q
python -m pytest tests/ -m asyncio -q
python -m pytest tests/ -m concurrent -q

# Coverage across all tests
python -m pytest tests/ --cov=src --cov-report=term-missing -q
```
</verification>

<success_criteria>
- Integration pipeline test runs discovery -> validation -> ranking with mocked API, produces correctly ranked SuburbReport objects
- SSE endpoint tests verify event streaming, completion sentinel, and connection cleanup after disconnect
- Thread safety tests verify 10 concurrent cache writes produce no corruption, server state stays consistent, queue is safe, singleton returns same instance
- Full test suite: 50+ tests, 0 failures
- No real API calls made during any test
</success_criteria>

<output>
After completion, create `.planning/phases/05-comprehensive-testing/05-02-SUMMARY.md`
</output>
