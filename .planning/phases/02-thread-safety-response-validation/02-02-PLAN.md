---
phase: 02-thread-safety-response-validation
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/research/validation.py
  - src/research/suburb_discovery.py
  - src/research/suburb_research.py
autonomous: true

must_haves:
  truths:
    - "A Perplexity response with a string price like '450000' instead of integer is coerced and accepted, not rejected"
    - "A response missing required fields (e.g., no median_price) produces a structured warning with field-level detail"
    - "All API responses are validated against Pydantic schemas before being cached"
    - "Validation failures for optional fields produce warnings but do not block processing"
  artifacts:
    - path: "src/research/validation.py"
      provides: "Pydantic validation schemas for discovery and research API responses"
      contains: "DiscoverySuburbResponse"
    - path: "src/research/suburb_discovery.py"
      provides: "Discovery response validation before caching"
      contains: "validate_discovery_response"
    - path: "src/research/suburb_research.py"
      provides: "Research response validation before caching"
      contains: "validate_research_response"
  key_links:
    - from: "src/research/suburb_discovery.py"
      to: "src/research/validation.py"
      via: "import and call validate_discovery_response before cache.put"
      pattern: "validate_discovery_response"
    - from: "src/research/suburb_research.py"
      to: "src/research/validation.py"
      via: "import and call validate_research_response before cache.put"
      pattern: "validate_research_response"
    - from: "src/research/validation.py"
      to: "src/security/exceptions.py"
      via: "raises ValidationError from exception hierarchy"
      pattern: "from security.exceptions import"
---

<objective>
Create Pydantic validation schemas for all API responses (Perplexity and Anthropic) and wire them into the discovery and research pipelines so malformed responses are caught before they enter the cache.

Purpose: Prevent corrupted or malformed API responses from being cached and silently producing bad reports. Flexible coercion handles LLM output variability (string numbers, missing optional fields) while structured warnings provide visibility into data quality.
Output: New validation.py module with Pydantic schemas, wired into discovery and research flows.
</objective>

<execution_context>
@/Users/robertli/.claude/get-shit-done/workflows/execute-plan.md
@/Users/robertli/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-thread-safety-response-validation/02-RESEARCH.md
@src/research/suburb_discovery.py
@src/research/suburb_research.py
@src/models/suburb_metrics.py
@src/security/exceptions.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create validation schemas with flexible coercion</name>
  <files>src/research/validation.py</files>
  <action>
Create `src/research/validation.py` with Pydantic v2 validation schemas for both discovery and research API responses.

1. **Numeric coercion helper** (BeforeValidator):
   ```python
   def coerce_numeric(value):
       """Coerce string numbers to int/float. Handles '450000', '3.5', etc."""
       if isinstance(value, str):
           try:
               return int(value) if '.' not in value else float(value)
           except ValueError:
               pass
       return value
   ```

2. **Discovery validation schema** -- `DiscoverySuburbResponse`:
   - `name: str` (required, min_length=1)
   - `state: str` (required, pattern `^(NSW|VIC|QLD|SA|WA|TAS|NT|ACT)$`)
   - `lga: str` (required, min_length=1)
   - `region: Optional[str] = None`
   - `median_price: Annotated[float, BeforeValidator(coerce_numeric)]` (required, gt=0)
   - `growth_signals: list[str] = Field(default_factory=list)`
   - `major_events_relevance: Optional[str] = None`
   - `data_quality: Optional[str] = Field(default="medium")` -- validate against "high|medium|low", default to "medium" if invalid

3. **Research validation schemas** -- nested models for each section:
   - `ResearchIdentificationResponse` -- name (required), state (required), lga (required), region (optional)
   - `ResearchMarketCurrentResponse` -- median_price (required, coerced), all others Optional with coercion
   - `ResearchMarketHistoryResponse` -- all list fields Optional with default empty lists
   - `ResearchPhysicalConfigResponse` -- all Optional with coercion
   - `ResearchDemographicsResponse` -- all Optional
   - `ResearchInfrastructureResponse` -- list fields default to empty lists, string fields Optional
   - `ResearchGrowthProjectionsResponse` -- projected_growth_pct as dict with string key coercion (`"1"` -> `1`), scores with coercion, all Optional with defaults
   - `ResearchSuburbResponse` -- top-level model combining all sections. `identification` and `market_current` are required. All other sections are Optional with defaults.

4. **Validation result type** -- `ValidationResult`:
   ```python
   @dataclass
   class ValidationResult:
       data: dict          # The validated/coerced data as a dict
       warnings: list[str] # Field-level warnings for missing optional data
       is_valid: bool      # True if required fields passed
   ```

5. **Validation functions**:
   - `validate_discovery_response(raw_list: list[dict]) -> ValidationResult`:
     - Validate each item in the list against `DiscoverySuburbResponse`
     - Items that pass: include in result with coerced values
     - Items that fail required fields: log structured warning with field-level detail from `ValidationError.errors()`, exclude from result
     - Return `ValidationResult` with valid items and per-item warnings

   - `validate_research_response(raw_data: dict, suburb_name: str) -> ValidationResult`:
     - Validate against `ResearchSuburbResponse`
     - Required fields failing: raise `ValidationError` from `security.exceptions` with field-level detail
     - Optional fields missing: add to warnings list (e.g., "market_history: No historical data available")
     - Return `ValidationResult` with coerced data and warnings

6. **Warning format**: Each warning should be a string like `"field_name: description (got: actual_value)"` for machine and human readability.

7. **Import from security.exceptions**: Use `from security.exceptions import ValidationError as AppValidationError` to avoid collision with Pydantic's `ValidationError`. Raise `AppValidationError` for fatal validation failures (required fields missing).

Use Pydantic v2 default lax mode (NOT strict=True) -- this automatically handles string-to-number coercion for most cases. Add explicit `BeforeValidator(coerce_numeric)` only for fields where LLMs commonly return strings (median_price, scores, numeric config values).
  </action>
  <verify>
Run from `src/` directory:
```
python -c "
from research.validation import validate_discovery_response, validate_research_response, ValidationResult

# Test string coercion
result = validate_discovery_response([{'name': 'TestSuburb', 'state': 'QLD', 'lga': 'Brisbane', 'median_price': '450000', 'growth_signals': ['test']}])
assert result.is_valid
assert result.data[0]['median_price'] == 450000.0
print('OK: string price coerced to float')

# Test missing required field
result2 = validate_discovery_response([{'name': '', 'state': 'QLD', 'lga': 'Brisbane', 'median_price': 0}])
assert len(result2.warnings) > 0
print('OK: missing required field produces warning')

print('All validation tests passed')
"
```
  </verify>
  <done>
`src/research/validation.py` exists with `DiscoverySuburbResponse`, `ResearchSuburbResponse` schemas, `coerce_numeric` helper, `validate_discovery_response()` and `validate_research_response()` functions. String prices coerced to numbers. Missing required fields produce structured field-level warnings. Missing optional fields produce warnings but do not block processing.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire validation into discovery and research pipelines</name>
  <files>src/research/suburb_discovery.py, src/research/suburb_research.py</files>
  <action>
Wire the validation schemas into both pipelines so API responses are validated BEFORE being cached.

**In `src/research/suburb_discovery.py`**:

1. Add import: `from research.validation import validate_discovery_response`

2. In `discover_suburbs()` function, after parsing JSON and before caching (around line 180-181):
   - After `raw_list` is constructed from the API response, call `validate_discovery_response(raw_list)`
   - If validation result has warnings, log each warning at WARNING level with suburb name
   - Replace `raw_list` with the validated/coerced data: `raw_list = validation_result.data`
   - Then cache the validated data: `cache.put("discovery", raw_list, **cache_key_parts)`
   - This ensures only validated data enters the cache

3. Also validate cached data on read (around line 142):
   - After reading from cache (`if cached is not None:`), validate with `validate_discovery_response(cached)`
   - If cached data fails validation, invalidate cache and re-fetch (log warning: "Cached discovery data failed validation, re-fetching")

**In `src/research/suburb_research.py`**:

1. Add import: `from research.validation import validate_research_response`

2. In `research_suburb()` function, after parsing JSON and before caching (around line 193-194):
   - After `data = client.parse_json_response(response)`, call `validate_research_response(data, candidate.name)`
   - If validation result has warnings, log each warning at WARNING level
   - Replace `data` with validated/coerced data: `data = validation_result.data`
   - Then cache the validated data: `cache.put("research", data, **cache_key_parts)`
   - If validation raises AppValidationError (required fields missing), catch it and fall back to `_create_fallback_metrics(candidate)` with a log warning

3. Also validate cached data on read (around line 164-174):
   - After reading from cache and before `_parse_metrics_from_json(cached)`, validate with `validate_research_response(cached, candidate.name)`
   - If cached data fails validation, invalidate and re-fetch

Keep the existing `_parse_metrics_from_json()` function -- validation runs BEFORE it. The validation schemas ensure data shape is correct; `_parse_metrics_from_json()` converts to domain models. Do not duplicate the conversion logic.

The key flow becomes: API response -> JSON parse -> **validate & coerce** -> cache -> parse to domain models
  </action>
  <verify>
Run from `src/` directory:
```
python -c "
# Verify imports work
from research.suburb_discovery import discover_suburbs
from research.suburb_research import research_suburb
from research.validation import validate_discovery_response, validate_research_response
print('OK: all imports successful')

# Verify validation is referenced in the modules
import inspect
discovery_src = inspect.getsource(discover_suburbs)
assert 'validate_discovery_response' in discovery_src, 'Discovery not wired'
print('OK: discovery wired to validation')

research_src = inspect.getsource(research_suburb)
assert 'validate_research_response' in research_src, 'Research not wired'
print('OK: research wired to validation')
"
```
  </verify>
  <done>
Discovery pipeline validates API responses before caching via `validate_discovery_response()`. Research pipeline validates API responses before caching via `validate_research_response()`. Cached data is also validated on read. String prices are coerced. Missing optional fields produce logged warnings. Missing required fields trigger fallback metrics with structured warning.
  </done>
</task>

</tasks>

<verification>
1. `src/research/validation.py` exists with DiscoverySuburbResponse and ResearchSuburbResponse schemas
2. String price "450000" coerced to float 450000.0 (not rejected)
3. Missing required field (empty name, zero price) produces structured warning with field path
4. Missing optional fields (no market_history) produce warning but validation passes
5. Discovery pipeline calls validate_discovery_response before cache.put
6. Research pipeline calls validate_research_response before cache.put
7. Cached data validated on read -- invalid cache entries trigger re-fetch
</verification>

<success_criteria>
- Pydantic schemas with lax mode handle LLM output variability (string numbers, nulls, missing optional fields)
- All API responses validated before entering cache
- Validation failures for required fields produce AppValidationError with field-level detail
- Validation failures for optional fields produce warnings (not errors)
- Discovery and research pipelines both wire through validation
- No existing functionality broken -- validation is additive
</success_criteria>

<output>
After completion, create `.planning/phases/02-thread-safety-response-validation/02-02-SUMMARY.md`
</output>
