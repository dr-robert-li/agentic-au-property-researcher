---
phase: 02-thread-safety-response-validation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/research/cache.py
  - src/ui/web/server.py
autonomous: true

must_haves:
  truths:
    - "Cache singleton initialization from multiple threads always returns the same instance with no duplicate initialization"
    - "Two simultaneous research runs from the web UI never produce corrupted run state"
    - "Progress callbacks from background worker threads do not cause race conditions or deadlocks on shared state"
  artifacts:
    - path: "src/research/cache.py"
      provides: "Thread-safe double-checked locking singleton for cache"
      contains: "_cache_lock"
    - path: "src/ui/web/server.py"
      provides: "Lock-protected active_runs and completed_runs dicts, queue-based progress"
      contains: "active_runs_lock"
  key_links:
    - from: "src/ui/web/server.py"
      to: "src/research/cache.py"
      via: "get_cache() called from endpoints"
      pattern: "get_cache\\(\\)"
    - from: "src/ui/web/server.py"
      to: "queue.Queue"
      via: "progress_callback pushes to queue instead of mutating active_runs directly"
      pattern: "progress_queue"
---

<objective>
Make cache singleton initialization and web server shared state thread-safe to prevent race conditions when multiple research runs execute concurrently.

Purpose: Two simultaneous research runs must not corrupt shared state (active_runs, completed_runs dicts) or create duplicate cache instances. Python 3.14 free-threaded mode makes explicit locking essential.
Output: Thread-safe cache.py singleton and lock-protected server.py global state with queue-based progress reporting.
</objective>

<execution_context>
@/Users/robertli/.claude/get-shit-done/workflows/execute-plan.md
@/Users/robertli/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-thread-safety-response-validation/02-RESEARCH.md
@src/research/cache.py
@src/ui/web/server.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Thread-safe cache singleton with double-checked locking</name>
  <files>src/research/cache.py</files>
  <action>
Modify the `get_cache()` function (lines 300-312) and `reset_cache_instance()` (lines 315-318) in `src/research/cache.py` to use thread-safe double-checked locking pattern:

1. Add a module-level `_cache_lock = threading.Lock()` next to the existing `_cache_instance` variable (line 297).

2. Rewrite `get_cache()` to use double-checked locking:
   - First check: `if _cache_instance is not None: return _cache_instance` (fast path, no lock)
   - Acquire `_cache_lock`
   - Second check inside lock: `if _cache_instance is None:` then initialize
   - This prevents two threads from both seeing None and creating duplicate instances

3. Update `reset_cache_instance()` to also acquire `_cache_lock` before setting `_cache_instance = None` (for test safety).

Do NOT change the ResearchCache class itself -- it already uses `self._lock = threading.RLock()` for its operations. Only the module-level singleton access pattern needs fixing.

Pattern reference (from research):
```python
_cache_instance: Optional[ResearchCache] = None
_cache_lock = threading.Lock()

def get_cache() -> ResearchCache:
    global _cache_instance
    if _cache_instance is not None:
        return _cache_instance
    with _cache_lock:
        if _cache_instance is None:
            from config import settings
            config = CacheConfig(
                cache_dir=settings.CACHE_DIR,
                discovery_ttl=settings.CACHE_DISCOVERY_TTL,
                research_ttl=settings.CACHE_RESEARCH_TTL,
                enabled=settings.CACHE_ENABLED,
            )
            _cache_instance = ResearchCache(config)
    return _cache_instance
```
  </action>
  <verify>
Run: `python -c "from research.cache import get_cache, _cache_lock; import threading; assert isinstance(_cache_lock, type(threading.Lock())); c1 = get_cache(); c2 = get_cache(); assert c1 is c2; print('OK: singleton and lock verified')"` from `src/` directory.
  </verify>
  <done>
`get_cache()` uses double-checked locking with `_cache_lock`. Multiple calls return the same instance. `reset_cache_instance()` acquires lock before resetting.
  </done>
</task>

<task type="auto">
  <name>Task 2: Lock-protect web server global state and add progress queue</name>
  <files>src/ui/web/server.py</files>
  <action>
Modify `src/ui/web/server.py` to protect all access to `active_runs` and `completed_runs` dicts with threading locks, and switch progress reporting from direct dict mutation to `queue.Queue`.

1. **Add imports** at top: `import threading`, `import queue` (threading may already be imported indirectly; add explicit import).

2. **Add locks** after the global dict declarations (around line 103-104):
   ```python
   active_runs = {}
   completed_runs = {}
   active_runs_lock = threading.Lock()
   completed_runs_lock = threading.Lock()
   progress_queues: dict[str, queue.Queue] = {}
   ```

3. **Update `run_pipeline_background()`** (line 122):
   - Wrap ALL reads/writes to `active_runs` with `with active_runs_lock:`
   - Wrap ALL reads/writes to `completed_runs` with `with completed_runs_lock:`
   - Replace the `progress_callback` closure: instead of directly mutating `active_runs[run_id].setdefault("steps", []).append(...)`, create a `queue.Queue(maxsize=100)` stored in `progress_queues[run_id]`, and have the callback push messages to the queue via `put(timeout=1)` with `except queue.Full: pass`
   - When moving from active to completed: acquire `active_runs_lock` to pop, then acquire `completed_runs_lock` to store
   - In all three except blocks: same lock discipline for pop/store
   - After completion, put `None` sentinel on the progress queue

4. **Update `start_run()`** (line 231):
   - Wrap `active_runs[run_id] = {...}` with `with active_runs_lock:`

5. **Update `run_status()`** (line 275):
   - Wrap `if run_id in active_runs` check with `with active_runs_lock:` and deepcopy the result
   - Wrap `if run_id in completed_runs` check with `with completed_runs_lock:`

6. **Update `api_run_status()`** (line 295):
   - Wrap `active_runs` access with `with active_runs_lock:` and `copy.deepcopy()`
   - Wrap `completed_runs` access with `with completed_runs_lock:`

7. **Update `list_runs()`** (line 312):
   - The `active_runs` reference passed to template should be a deepcopy under lock:
     ```python
     with active_runs_lock:
         active_runs_snapshot = copy.deepcopy(active_runs)
     ```

8. **Update `health_check()`** (line 489):
   - Wrap `len(active_runs)` and `len(completed_runs)` with their respective locks

9. **Add progress endpoint** (new):
   ```python
   @app.get("/api/progress/{run_id}")
   async def api_run_progress(run_id: str):
       progress_queue = progress_queues.get(run_id)
       if not progress_queue:
           return JSONResponse(content={"progress": [], "done": True})
       messages = []
       done = False
       try:
           while True:
               msg = progress_queue.get_nowait()
               if msg is None:
                   done = True
                   break
               messages.append(msg)
       except queue.Empty:
           pass
       return JSONResponse(content={"progress": messages, "done": done})
   ```

IMPORTANT: Use `threading.Lock()` not `threading.RLock()` -- no recursive acquisition needed (per research recommendation). Use `copy.deepcopy()` when returning mutable state to prevent cross-thread mutations during JSON serialization.
  </action>
  <verify>
Run: `python -c "from ui.web.server import active_runs_lock, completed_runs_lock, progress_queues; import threading, queue; assert isinstance(active_runs_lock, type(threading.Lock())); assert isinstance(completed_runs_lock, type(threading.Lock())); assert isinstance(progress_queues, dict); print('OK: locks and progress_queues verified')"` from `src/` directory.
  </verify>
  <done>
All `active_runs` and `completed_runs` access is wrapped with respective locks. Progress callbacks use `queue.Queue` instead of direct dict mutation. New `/api/progress/{run_id}` endpoint drains progress queue. `copy.deepcopy()` used for all cross-thread state reads.
  </done>
</task>

</tasks>

<verification>
1. `get_cache()` returns the same instance when called from multiple threads
2. `active_runs_lock` and `completed_runs_lock` exist as `threading.Lock` instances
3. All `active_runs` and `completed_runs` access sites are wrapped with locks
4. Progress callbacks use `queue.Queue` not direct dict mutation
5. `/api/progress/{run_id}` endpoint exists and returns JSON
6. No deadlock risk: locks are never held recursively, never nested (active_runs_lock and completed_runs_lock acquired separately, not simultaneously)
</verification>

<success_criteria>
- Cache singleton uses double-checked locking pattern with `_cache_lock`
- Web server `active_runs` and `completed_runs` protected by `threading.Lock`
- Progress reporting uses `queue.Queue` per run
- No recursive lock acquisition or nested lock patterns that could deadlock
- All changes use `with lock:` context managers (never manual acquire/release)
</success_criteria>

<output>
After completion, create `.planning/phases/02-thread-safety-response-validation/02-01-SUMMARY.md`
</output>
